# 方案4-混合架构优化方案设计-技术实现细节补充

## 1. 异步处理框架选择与实现

### 1.1 异步框架选择决策

**框架对比分析**：
| 框架 | 适用场景 | 优势 | 劣势 | 推荐度 |
|------|---------|------|------|--------|
| asyncio | I/O密集型操作 | 原生支持、性能优秀 | 学习曲线陡峭 | ⭐⭐⭐⭐⭐ |
| threading | CPU密集型操作 | 简单易用、兼容性好 | GIL限制、资源消耗大 | ⭐⭐⭐ |
| multiprocessing | 真正并行计算 | 绕过GIL、性能最佳 | 进程开销大、通信复杂 | ⭐⭐ |

**最终选择**：**asyncio + threading混合模式**
- 文件I/O操作使用asyncio（异步）
- 计算密集型操作使用threading（线程池）
- 大文件处理使用multiprocessing（进程池）

### 1.2 具体实现方案

#### 1.2.1 异步文件I/O实现
```python
import asyncio
import aiofiles
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from typing import Union, Optional

class HybridAsyncProcessor:
    def __init__(self):
        self.io_executor = ThreadPoolExecutor(max_workers=4)  # I/O线程池
        self.cpu_executor = ProcessPoolExecutor(max_workers=2)  # CPU进程池
        self.chunk_size = 1024 * 1024  # 1MB chunks
    
    async def read_file_async(self, file_path: Path) -> str:
        """异步读取文件内容"""
        try:
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                content = await f.read()
                return content
        except Exception as e:
            raise FileReadError(f"Failed to read file {file_path}: {e}")
    
    async def read_file_chunked_async(self, file_path: Path) -> AsyncGenerator[bytes, None]:
        """异步分块读取大文件"""
        async with aiofiles.open(file_path, 'rb') as f:
            while True:
                chunk = await f.read(self.chunk_size)
                if not chunk:
                    break
                yield chunk
    
    async def process_file_hybrid(self, file_path: Path, processor: Callable) -> Any:
        """混合模式处理文件"""
        # 小文件：异步I/O + 线程池处理
        if file_path.stat().st_size < 10 * 1024 * 1024:  # < 10MB
            content = await self.read_file_async(file_path)
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                self.io_executor, processor, content
            )
            return result
        
        # 大文件：异步I/O + 进程池处理
        else:
            chunks = []
            async for chunk in self.read_file_chunked_async(file_path):
                chunks.append(chunk)
            
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                self.cpu_executor, self._process_chunks, chunks, processor
            )
            return result
    
    def _process_chunks(self, chunks: List[bytes], processor: Callable) -> Any:
        """处理文件块"""
        # 在进程池中执行
        return processor(b''.join(chunks))
```

#### 1.2.2 线程池与进程池配置
```python
class ExecutorConfig:
    """执行器配置管理"""
    
    def __init__(self):
        self.io_workers = min(4, (os.cpu_count() or 1) * 2)
        self.cpu_workers = max(1, (os.cpu_count() or 1) - 1)
        self.max_memory_mb = 512  # 最大内存限制
    
    def get_io_executor(self) -> ThreadPoolExecutor:
        """获取I/O执行器"""
        return ThreadPoolExecutor(
            max_workers=self.io_workers,
            thread_name_prefix="IO-Worker"
        )
    
    def get_cpu_executor(self) -> ProcessPoolExecutor:
        """获取CPU执行器"""
        return ProcessPoolExecutor(
            max_workers=self.cpu_workers,
            mp_context=mp.get_context('spawn')  # 使用spawn模式提高稳定性
        )
    
    def get_memory_aware_executor(self) -> ProcessPoolExecutor:
        """获取内存感知执行器"""
        return ProcessPoolExecutor(
            max_workers=self._calculate_memory_workers(),
            mp_context=mp.get_context('spawn')
        )
    
    def _calculate_memory_workers(self) -> int:
        """根据可用内存计算进程数"""
        import psutil
        available_memory = psutil.virtual_memory().available
        memory_per_worker = self.max_memory_mb * 1024 * 1024
        return max(1, min(self.cpu_workers, available_memory // memory_per_worker))
```

### 1.3 缓存一致性策略实现

#### 1.3.1 分布式缓存一致性保证
```python
from typing import Dict, Any, Optional, List
import hashlib
import time
import threading

class DistributedCacheManager:
    """分布式缓存管理器"""
    
    def __init__(self):
        self.local_cache = {}
        self.remote_caches = {}  # 其他节点的缓存
        self.consistency_locks = {}
        self.version_vector = {}  # 版本向量
    
    def set_with_consistency(self, key: str, value: Any, 
                           consistency_level: str = 'strong') -> bool:
        """设置缓存值，保证一致性"""
        if consistency_level == 'strong':
            return self._set_strong_consistency(key, value)
        elif consistency_level == 'eventual':
            return self._set_eventual_consistency(key, value)
        else:
            return self._set_weak_consistency(key, value)
    
    def _set_strong_consistency(self, key: str, value: Any) -> bool:
        """强一致性设置"""
        # 获取分布式锁
        lock = self._acquire_distributed_lock(key)
        if not lock:
            return False
        
        try:
            # 更新版本向量
            self._update_version_vector(key)
            
            # 同步更新所有节点
            success = self._sync_update_all_nodes(key, value)
            
            if success:
                # 更新本地缓存
                self.local_cache[key] = {
                    'value': value,
                    'timestamp': time.time(),
                    'version': self.version_vector.get(key, 0)
                }
                return True
            
            return False
        finally:
            self._release_distributed_lock(key, lock)
    
    def _set_eventual_consistency(self, key: str, value: Any) -> bool:
        """最终一致性设置"""
        # 异步更新，不等待所有节点确认
        self.local_cache[key] = {
            'value': value,
            'timestamp': time.time(),
            'version': self.version_vector.get(key, 0)
        }
        
        # 异步同步到其他节点
        asyncio.create_task(self._async_sync_nodes(key, value))
        return True
    
    def _set_weak_consistency(self, key: str, value: Any) -> bool:
        """弱一致性设置"""
        # 只更新本地缓存
        self.local_cache[key] = {
            'value': value,
            'timestamp': time.time(),
            'version': self.version_vector.get(key, 0)
        }
        return True
    
    def get_with_consistency(self, key: str, 
                           consistency_level: str = 'eventual') -> Optional[Any]:
        """获取缓存值，保证一致性"""
        if consistency_level == 'strong':
            return self._get_strong_consistency(key)
        elif consistency_level == 'eventual':
            return self._get_eventual_consistency(key)
        else:
            return self._get_weak_consistency(key)
    
    def _get_strong_consistency(self, key: str) -> Optional[Any]:
        """强一致性读取"""
        # 检查所有节点的版本
        max_version = self._get_max_version_across_nodes(key)
        local_version = self.version_vector.get(key, 0)
        
        if local_version < max_version:
            # 本地版本落后，需要同步
            self._sync_from_remote_nodes(key)
        
        return self.local_cache.get(key, {}).get('value')
    
    def _get_eventual_consistency(self, key: str) -> Optional[Any]:
        """最终一致性读取"""
        # 检查本地缓存
        local_value = self.local_cache.get(key, {}).get('value')
        
        # 异步检查远程节点是否有更新
        asyncio.create_task(self._async_check_remote_updates(key))
        
        return local_value
    
    def _get_weak_consistency(self, key: str) -> Optional[Any]:
        """弱一致性读取"""
        return self.local_cache.get(key, {}).get('value')
    
    def _acquire_distributed_lock(self, key: str) -> Optional[str]:
        """获取分布式锁"""
        lock_id = hashlib.md5(f"{key}_{time.time()}".encode()).hexdigest()
        
        # 尝试在所有节点上获取锁
        for node_id, node_cache in self.remote_caches.items():
            if not node_cache.try_lock(key, lock_id):
                return None
        
        return lock_id
    
    def _release_distributed_lock(self, key: str, lock_id: str):
        """释放分布式锁"""
        for node_id, node_cache in self.remote_caches.items():
            node_cache.release_lock(key, lock_id)
    
    def _update_version_vector(self, key: str):
        """更新版本向量"""
        current_version = self.version_vector.get(key, 0)
        self.version_vector[key] = current_version + 1
    
    def _sync_update_all_nodes(self, key: str, value: Any) -> bool:
        """同步更新所有节点"""
        success_count = 0
        total_nodes = len(self.remote_caches) + 1  # 包括本地节点
        
        for node_id, node_cache in self.remote_caches.items():
            try:
                if node_cache.set(key, value, timeout=5.0):
                    success_count += 1
            except Exception as e:
                logging.warning(f"Failed to sync to node {node_id}: {e}")
        
        # 本地节点总是成功
        success_count += 1
        
        # 要求超过半数节点成功
        return success_count > total_nodes // 2
```

#### 1.3.2 缓存失效策略优化
```python
class AdvancedCacheInvalidator:
    """高级缓存失效器"""
    
    def __init__(self, cache_manager: DistributedCacheManager):
        self.cache_manager = cache_manager
        self.file_watchers = {}
        self.dependency_graph = {}
        self.invalidation_queue = asyncio.Queue()
        self.invalidation_worker = None
    
    async def start_invalidation_worker(self):
        """启动失效工作线程"""
        self.invalidation_worker = asyncio.create_task(
            self._invalidation_worker_loop()
        )
    
    async def _invalidation_worker_loop(self):
        """失效工作循环"""
        while True:
            try:
                invalidation_task = await self.invalidation_queue.get()
                await self._process_invalidation(invalidation_task)
                self.invalidation_queue.task_done()
            except Exception as e:
                logging.error(f"Invalidation worker error: {e}")
                await asyncio.sleep(1)
    
    async def _process_invalidation(self, task: Dict[str, Any]):
        """处理失效任务"""
        invalidation_type = task.get('type')
        
        if invalidation_type == 'file_change':
            await self._invalidate_file_caches(task['file_path'])
        elif invalidation_type == 'dependency_change':
            await self._invalidate_dependency_caches(task['dependency'])
        elif invalidation_type == 'time_based':
            await self._invalidate_time_based_caches(task['timestamp'])
        elif invalidation_type == 'memory_pressure':
            await self._invalidate_memory_pressure_caches()
    
    async def _invalidate_file_caches(self, file_path: Path):
        """文件变化导致的缓存失效"""
        # 获取所有相关的缓存键
        related_keys = self._get_related_cache_keys(file_path)
        
        # 批量失效
        for cache_type in ['render', 'content', 'file_info']:
            cache = self.cache_manager.get_cache(cache_type)
            if cache:
                for key in related_keys:
                    await cache.delete_async(key)
        
        # 通知其他节点
        await self._notify_remote_invalidation(file_path, related_keys)
    
    def _get_related_cache_keys(self, file_path: Path) -> List[str]:
        """获取文件相关的缓存键"""
        keys = []
        
        # 基于文件路径的键
        keys.append(f"file:{file_path.absolute()}")
        
        # 基于文件内容的键
        try:
            content_hash = self._calculate_file_hash(file_path)
            keys.append(f"content:{content_hash}")
        except Exception:
            pass
        
        # 基于依赖关系的键
        if file_path in self.dependency_graph:
            for dependent_key in self.dependency_graph[file_path]:
                keys.append(dependent_key)
        
        return keys
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """计算文件哈希值"""
        hasher = hashlib.sha256()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        return hasher.hexdigest()
    
    async def _notify_remote_invalidation(self, file_path: Path, keys: List[str]):
        """通知远程节点缓存失效"""
        invalidation_message = {
            'type': 'cache_invalidation',
            'file_path': str(file_path),
            'keys': keys,
            'timestamp': time.time()
        }
        
        for node_id, node_cache in self.cache_manager.remote_caches.items():
            try:
                await node_cache.receive_invalidation(invalidation_message)
            except Exception as e:
                logging.warning(f"Failed to notify node {node_id}: {e}")
    
    def watch_file(self, file_path: Path, cache_keys: List[str]):
        """监听文件变化"""
        if file_path not in self.file_watchers:
            watcher = FileWatcher(file_path)
            watcher.on_change = lambda: self._queue_file_invalidation(file_path)
            self.file_watchers[file_path] = watcher
            watcher.start()
        
        self.file_watchers[file_path].add_cache_keys(cache_keys)
    
    def _queue_file_invalidation(self, file_path: Path):
        """将文件失效任务加入队列"""
        task = {
            'type': 'file_change',
            'file_path': file_path,
            'timestamp': time.time()
        }
        
        # 异步加入队列
        asyncio.create_task(self.invalidation_queue.put(task))
```

### 1.4 错误恢复机制实现

#### 1.4.1 智能重试机制
```python
from typing import Callable, Any, Optional, Dict, List
import time
import random
import asyncio

class SmartRetryMechanism:
    """智能重试机制"""
    
    def __init__(self):
        self.retry_strategies = {
            'exponential_backoff': self._exponential_backoff_retry,
            'linear_backoff': self._linear_backoff_retry,
            'jitter_backoff': self._jitter_backoff_retry,
            'adaptive_retry': self._adaptive_retry
        }
        self.retry_history = {}
        self.success_rates = {}
    
    async def execute_with_retry(self, 
                               operation: Callable,
                               operation_name: str,
                               max_retries: int = 3,
                               strategy: str = 'exponential_backoff',
                               **kwargs) -> Any:
        """执行操作并自动重试"""
        
        # 获取重试策略
        retry_func = self.retry_strategies.get(strategy, self._exponential_backoff_retry)
        
        # 初始化重试历史
        if operation_name not in self.retry_history:
            self.retry_history[operation_name] = []
        
        last_error = None
        retry_count = 0
        
        for attempt in range(max_retries + 1):
            try:
                # 执行操作
                if asyncio.iscoroutinefunction(operation):
                    result = await operation(**kwargs)
                else:
                    result = operation(**kwargs)
                
                # 记录成功
                self._record_success(operation_name, attempt)
                return result
                
            except Exception as e:
                last_error = e
                retry_count = attempt
                
                # 记录失败
                self._record_failure(operation_name, attempt, e)
                
                # 判断是否应该重试
                if not self._should_retry(e, attempt, max_retries):
                    break
                
                # 计算重试延迟
                delay = retry_func(attempt, operation_name)
                
                # 等待重试
                await asyncio.sleep(delay)
        
        # 所有重试都失败了
        raise RetryExhaustedError(
            f"Operation {operation_name} failed after {retry_count} retries. "
            f"Last error: {last_error}"
        )
    
    def _exponential_backoff_retry(self, attempt: int, operation_name: str) -> float:
        """指数退避重试策略"""
        base_delay = 1.0  # 基础延迟1秒
        max_delay = 60.0   # 最大延迟60秒
        
        delay = min(base_delay * (2 ** attempt), max_delay)
        
        # 添加随机抖动
        jitter = random.uniform(0, delay * 0.1)
        return delay + jitter
    
    def _linear_backoff_retry(self, attempt: int, operation_name: str) -> float:
        """线性退避重试策略"""
        base_delay = 1.0  # 基础延迟1秒
        max_delay = 30.0   # 最大延迟30秒
        
        delay = min(base_delay * (attempt + 1), max_delay)
        return delay
    
    def _jitter_backoff_retry(self, attempt: int, operation_name: str) -> float:
        """抖动退避重试策略"""
        base_delay = 1.0
        max_delay = 60.0
        
        # 使用随机抖动
        delay = random.uniform(base_delay, max_delay)
        return delay
    
    def _adaptive_retry(self, attempt: int, operation_name: str) -> float:
        """自适应重试策略"""
        # 基于历史成功率调整延迟
        success_rate = self.success_rates.get(operation_name, 0.5)
        
        if success_rate > 0.8:
            # 高成功率，使用较短延迟
            base_delay = 0.5
        elif success_rate > 0.5:
            # 中等成功率，使用中等延迟
            base_delay = 1.0
        else:
            # 低成功率，使用较长延迟
            base_delay = 2.0
        
        delay = base_delay * (2 ** attempt)
        return min(delay, 60.0)
    
    def _should_retry(self, error: Exception, attempt: int, max_retries: int) -> bool:
        """判断是否应该重试"""
        # 检查重试次数
        if attempt >= max_retries:
            return False
        
        # 检查错误类型
        if isinstance(error, (ValueError, TypeError, AttributeError)):
            # 编程错误，不应该重试
            return False
        
        if isinstance(error, (FileNotFoundError, PermissionError)):
            # 文件系统错误，可以重试
            return True
        
        if isinstance(error, (ConnectionError, TimeoutError)):
            # 网络错误，可以重试
            return True
        
        # 默认可以重试
        return True
    
    def _record_success(self, operation_name: str, attempt: int):
        """记录成功"""
        if operation_name not in self.retry_history:
            self.retry_history[operation_name] = []
        
        self.retry_history[operation_name].append({
            'attempt': attempt,
            'success': True,
            'timestamp': time.time()
        })
        
        # 更新成功率
        self._update_success_rate(operation_name)
    
    def _record_failure(self, operation_name: str, attempt: int, error: Exception):
        """记录失败"""
        if operation_name not in self.retry_history:
            self.retry_history[operation_name] = []
        
        self.retry_history[operation_name].append({
            'attempt': attempt,
            'success': False,
            'error_type': type(error).__name__,
            'error_message': str(error),
            'timestamp': time.time()
        })
        
        # 更新成功率
        self._update_success_rate(operation_name)
    
    def _update_success_rate(self, operation_name: str):
        """更新成功率"""
        history = self.retry_history.get(operation_name, [])
        if not history:
            return
        
        # 只考虑最近100次操作
        recent_history = history[-100:]
        success_count = sum(1 for record in recent_history if record['success'])
        total_count = len(recent_history)
        
        self.success_rates[operation_name] = success_count / total_count
    
    def get_retry_statistics(self, operation_name: str) -> Dict[str, Any]:
        """获取重试统计信息"""
        history = self.retry_history.get(operation_name, [])
        if not history:
            return {}
        
        total_attempts = len(history)
        successful_attempts = sum(1 for record in history if record['success'])
        failed_attempts = total_attempts - successful_attempts
        
        # 计算平均重试次数
        avg_retries = sum(record['attempt'] for record in history) / total_attempts
        
        # 计算成功率
        success_rate = successful_attempts / total_attempts if total_attempts > 0 else 0
        
        return {
            'total_attempts': total_attempts,
            'successful_attempts': successful_attempts,
            'failed_attempts': failed_attempts,
            'success_rate': success_rate,
            'average_retries': avg_retries,
            'recent_success_rate': self.success_rates.get(operation_name, 0)
        }
```

#### 1.4.2 降级策略实现
```python
class GracefulDegradationManager:
    """优雅降级管理器"""
    
    def __init__(self):
        self.degradation_strategies = {}
        self.fallback_handlers = {}
        self.health_checks = {}
        self.degradation_history = []
    
    def register_degradation_strategy(self, 
                                    component_name: str,
                                    strategy: DegradationStrategy):
        """注册降级策略"""
        self.degradation_strategies[component_name] = strategy
    
    def register_fallback_handler(self,
                                component_name: str,
                                handler: Callable):
        """注册降级处理器"""
        self.fallback_handlers[component_name] = handler
    
    def register_health_check(self,
                            component_name: str,
                            health_check: Callable):
        """注册健康检查"""
        self.health_checks[component_name] = health_check
    
    async def execute_with_graceful_degradation(self,
                                              component_name: str,
                                              operation: Callable,
                                              **kwargs) -> Any:
        """执行操作，支持优雅降级"""
        
        # 检查组件健康状态
        if not await self._is_component_healthy(component_name):
            # 组件不健康，直接使用降级策略
            return await self._execute_fallback(component_name, **kwargs)
        
        try:
            # 尝试正常执行
            if asyncio.iscoroutinefunction(operation):
                result = await operation(**kwargs)
            else:
                result = operation(**kwargs)
            
            # 记录成功
            self._record_operation_result(component_name, True, None)
            return result
            
        except Exception as e:
            # 记录失败
            self._record_operation_result(component_name, False, e)
            
            # 执行降级策略
            return await self._execute_degradation_strategy(component_name, e, **kwargs)
    
    async def _is_component_healthy(self, component_name: str) -> bool:
        """检查组件健康状态"""
        health_check = self.health_checks.get(component_name)
        if not health_check:
            return True  # 没有健康检查，假设健康
        
        try:
            if asyncio.iscoroutinefunction(health_check):
                return await health_check()
            else:
                return health_check()
        except Exception:
            return False
    
    async def _execute_degradation_strategy(self,
                                          component_name: str,
                                          error: Exception,
                                          **kwargs) -> Any:
        """执行降级策略"""
        strategy = self.degradation_strategies.get(component_name)
        if not strategy:
            # 没有降级策略，使用默认降级
            return await self._execute_default_degradation(component_name, error, **kwargs)
        
        # 执行自定义降级策略
        try:
            if asyncio.iscoroutinefunction(strategy.execute):
                result = await strategy.execute(error, **kwargs)
            else:
                result = strategy.execute(error, **kwargs)
            
            # 记录降级成功
            self._record_degradation_result(component_name, True, None)
            return result
            
        except Exception as e:
            # 降级策略也失败了，使用默认降级
            self._record_degradation_result(component_name, False, e)
            return await self._execute_default_degradation(component_name, error, **kwargs)
    
    async def _execute_default_degradation(self,
                                         component_name: str,
                                         error: Exception,
                                         **kwargs) -> Any:
        """执行默认降级策略"""
        # 尝试使用降级处理器
        fallback_handler = self.fallback_handlers.get(component_name)
        if fallback_handler:
            try:
                if asyncio.iscoroutinefunction(fallback_handler):
                    result = await fallback_handler(error, **kwargs)
                else:
                    result = fallback_handler(error, **kwargs)
                
                # 记录降级成功
                self._record_degradation_result(component_name, True, None)
                return result
                
            except Exception as e:
                # 降级处理器也失败了
                self._record_degradation_result(component_name, False, e)
        
        # 所有降级策略都失败了，抛出错误
        raise DegradationFailedError(
            f"All degradation strategies failed for component {component_name}. "
            f"Original error: {error}"
        )
    
    def _record_operation_result(self, component_name: str, success: bool, error: Exception):
        """记录操作结果"""
        self.degradation_history.append({
            'component': component_name,
            'type': 'operation',
            'success': success,
            'error': error,
            'timestamp': time.time()
        })
    
    def _record_degradation_result(self, component_name: str, success: bool, error: Exception):
        """记录降级结果"""
        self.degradation_history.append({
            'component': component_name,
            'type': 'degradation',
            'success': success,
            'error': error,
            'timestamp': time.time()
        })
    
    def get_degradation_statistics(self, component_name: str = None) -> Dict[str, Any]:
        """获取降级统计信息"""
        if component_name:
            history = [record for record in self.degradation_history 
                      if record['component'] == component_name]
        else:
            history = self.degradation_history
        
        if not history:
            return {}
        
        # 计算成功率
        total_operations = len([r for r in history if r['type'] == 'operation'])
        successful_operations = len([r for r in history 
                                   if r['type'] == 'operation' and r['success']])
        
        total_degradations = len([r for r in history if r['type'] == 'degradation'])
        successful_degradations = len([r for r in history 
                                     if r['type'] == 'degradation' and r['success']])
        
        operation_success_rate = (successful_operations / total_operations 
                                if total_operations > 0 else 0)
        degradation_success_rate = (successful_degradations / total_degradations 
                                  if total_degradations > 0 else 0)
        
        return {
            'total_operations': total_operations,
            'successful_operations': successful_operations,
            'operation_success_rate': operation_success_rate,
            'total_degradations': total_degradations,
            'successful_degradations': successful_degradations,
            'degradation_success_rate': degradation_success_rate,
            'overall_success_rate': (successful_operations + successful_degradations) / 
                                  (total_operations + total_degradations) if (total_operations + total_degradations) > 0 else 0
        }
```

## 2. 性能基准完善

### 2.1 性能测试工具配置

#### 2.1.1 性能测试框架
```python
import cProfile
import pstats
import time
import memory_profiler
import psutil
from typing import Dict, Any, List, Callable

class PerformanceBenchmarkFramework:
    """性能基准测试框架"""
    
    def __init__(self):
        self.profiler = cProfile.Profile()
        self.memory_profiler = memory_profiler.profile
        self.benchmark_results = {}
        self.baseline_metrics = {}
    
    def benchmark_function(self, 
                          func: Callable,
                          func_name: str,
                          iterations: int = 1000,
                          warmup_iterations: int = 100,
                          **kwargs) -> Dict[str, Any]:
        """基准测试函数性能"""
        
        # 预热
        for _ in range(warmup_iterations):
            try:
                func(**kwargs)
            except Exception:
                pass
        
        # 开始性能分析
        self.profiler.enable()
        start_time = time.perf_counter()
        start_memory = psutil.Process().memory_info().rss
        
        # 执行函数
        for _ in range(iterations):
            try:
                result = func(**kwargs)
            except Exception as e:
                result = None
        
        # 结束性能分析
        end_time = time.perf_counter()
        end_memory = psutil.Process().memory_info().rss
        self.profiler.disable()
        
        # 获取性能统计
        stats = pstats.Stats(self.profiler)
        
        # 计算性能指标
        total_time = end_time - start_time
        avg_time = total_time / iterations
        memory_delta = end_memory - start_memory
        
        # 分析调用统计
        call_stats = self._analyze_call_stats(stats)
        
        # 记录结果
        benchmark_result = {
            'function_name': func_name,
            'iterations': iterations,
            'total_time': total_time,
            'average_time': avg_time,
            'memory_delta': memory_delta,
            'calls_per_second': iterations / total_time if total_time > 0 else 0,
            'call_statistics': call_stats,
            'timestamp': time.time()
        }
        
        self.benchmark_results[func_name] = benchmark_result
        return benchmark_result
    
    def _analyze_call_stats(self, stats: pstats.Stats) -> Dict[str, Any]:
        """分析调用统计"""
        # 获取前10个最耗时的函数调用
        stats.sort_stats('cumulative')
        
        call_stats = []
        for func, (cc, nc, tt, ct, callers) in stats.stats.items():
            if tt > 0:  # 只记录有执行时间的函数
                call_stats.append({
                    'function': f"{func[0]}:{func[1]}:{func[2]}",
                    'call_count': nc,
                    'total_time': tt,
                    'cumulative_time': ct,
                    'time_per_call': tt / nc if nc > 0 else 0
                })
        
        # 按累计时间排序，取前10个
        call_stats.sort(key=lambda x: x['cumulative_time'], reverse=True)
        return call_stats[:10]
    
    def benchmark_memory_usage(self, 
                              func: Callable,
                              func_name: str,
                              **kwargs) -> Dict[str, Any]:
        """基准测试内存使用"""
        
        # 使用memory_profiler装饰器
        profiled_func = self.memory_profiler(func)
        
        # 执行函数
        start_memory = psutil.Process().memory_info().rss
        result = profiled_func(**kwargs)
        end_memory = psutil.Process().memory_info().rss
        
        memory_result = {
            'function_name': func_name,
            'start_memory': start_memory,
            'end_memory': end_memory,
            'memory_delta': end_memory - start_memory,
            'result': result
        }
        
        return memory_result
    
    def compare_with_baseline(self, func_name: str) -> Dict[str, Any]:
        """与基线比较性能"""
        current_result = self.benchmark_results.get(func_name)
        baseline_result = self.baseline_metrics.get(func_name)
        
        if not current_result or not baseline_result:
            return {}
        
        # 计算性能变化
        time_change = ((current_result['average_time'] - baseline_result['average_time']) / 
                      baseline_result['average_time'] * 100)
        memory_change = ((current_result['memory_delta'] - baseline_result['memory_delta']) / 
                        baseline_result['memory_delta'] * 100)
        
        return {
            'function_name': func_name,
            'time_change_percent': time_change,
            'memory_change_percent': memory_change,
            'performance_improved': time_change < 0,
            'memory_improved': memory_change < 0,
            'current_metrics': current_result,
            'baseline_metrics': baseline_result
        }
    
    def set_baseline(self, func_name: str):
        """设置性能基线"""
        if func_name in self.benchmark_results:
            self.baseline_metrics[func_name] = self.benchmark_results[func_name].copy()
    
    def generate_performance_report(self) -> str:
        """生成性能报告"""
        report_lines = []
        report_lines.append("=" * 60)
        report_lines.append("性能基准测试报告")
        report_lines.append("=" * 60)
        
        for func_name, result in self.benchmark_results.items():
            report_lines.append(f"\n函数: {func_name}")
            report_lines.append(f"  执行次数: {result['iterations']}")
            report_lines.append(f"  总时间: {result['total_time']:.4f}秒")
            report_lines.append(f"  平均时间: {result['average_time']:.6f}秒")
            report_lines.append(f"  每秒调用次数: {result['calls_per_second']:.2f}")
            report_lines.append(f"  内存变化: {result['memory_delta'] / 1024 / 1024:.2f}MB")
            
            # 与基线比较
            comparison = self.compare_with_baseline(func_name)
            if comparison:
                report_lines.append(f"  性能变化: {comparison['time_change_percent']:+.2f}%")
                report_lines.append(f"  内存变化: {comparison['memory_change_percent']:+.2f}%")
        
        return "\n".join(report_lines)
```

#### 2.1.2 具体性能测试场景
```python
class PerformanceTestScenarios:
    """性能测试场景"""
    
    def __init__(self, benchmark_framework: PerformanceBenchmarkFramework):
        self.benchmark = benchmark_framework
        self.test_files = self._prepare_test_files()
    
    def _prepare_test_files(self) -> Dict[str, Path]:
        """准备测试文件"""
        test_files = {}
        
        # 小文件测试（< 100KB）
        test_files['small'] = self._create_test_file('small.md', 50 * 1024)
        
        # 中文件测试（100KB - 1MB）
        test_files['medium'] = self._create_test_file('medium.md', 500 * 1024)
        
        # 大文件测试（1MB - 10MB）
        test_files['large'] = self._create_test_file('large.md', 5 * 1024 * 1024)
        
        # 超大文件测试（> 10MB）
        test_files['extra_large'] = self._create_test_file('extra_large.md', 20 * 1024 * 1024)
        
        return test_files
    
    def _create_test_file(self, filename: str, size_bytes: int) -> Path:
        """创建测试文件"""
        file_path = Path(f"test_data/{filename}")
        file_path.parent.mkdir(exist_ok=True)
        
        # 生成测试内容
        content = self._generate_test_content(size_bytes)
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        return file_path
    
    def _generate_test_content(self, size_bytes: int) -> str:
        """生成测试内容"""
        # 生成Markdown格式的测试内容
        lines = []
        lines.append("# 性能测试文档")
        lines.append("")
        
        # 添加各种Markdown元素
        for i in range(size_bytes // 100):  # 每行约100字节
            if i % 10 == 0:
                lines.append(f"## 章节 {i // 10 + 1}")
                lines.append("")
            elif i % 5 == 0:
                lines.append(f"### 子章节 {i % 5 + 1}")
                lines.append("")
            
            lines.append(f"这是第 {i + 1} 行测试内容，包含一些 **粗体文本** 和 *斜体文本*。")
            lines.append("")
            
            if i % 20 == 0:
                lines.append("```python")
                lines.append(f"def test_function_{i}():")
                lines.append(f"    return 'test_{i}'")
                lines.append("```")
                lines.append("")
        
        content = "\n".join(lines)
        
        # 确保文件大小接近目标
        while len(content.encode('utf-8')) < size_bytes:
            content += "\n" + content
        
        return content[:size_bytes]
    
    def run_all_performance_tests(self) -> Dict[str, Any]:
        """运行所有性能测试"""
        test_results = {}
        
        # 测试文件读取性能
        test_results['file_reading'] = self._test_file_reading_performance()
        
        # 测试Markdown渲染性能
        test_results['markdown_rendering'] = self._test_markdown_rendering_performance()
        
        # 测试缓存性能
        test_results['caching'] = self._test_caching_performance()
        
        # 测试内存使用
        test_results['memory_usage'] = self._test_memory_usage()
        
        # 测试并发性能
        test_results['concurrency'] = self._test_concurrency_performance()
        
        return test_results
    
    def _test_file_reading_performance(self) -> Dict[str, Any]:
        """测试文件读取性能"""
        results = {}
        
        for file_size, file_path in self.test_files.items():
            # 测试同步读取
            sync_result = self.benchmark.benchmark_function(
                self._read_file_sync,
                f"read_file_sync_{file_size}",
                iterations=100,
                file_path=file_path
            )
            
            # 测试异步读取
            async_result = self.benchmark.benchmark_function(
                self._read_file_async,
                f"read_file_async_{file_size}",
                iterations=100,
                file_path=file_path
            )
            
            results[file_size] = {
                'sync': sync_result,
                'async': async_result,
                'improvement': ((sync_result['average_time'] - async_result['average_time']) / 
                              sync_result['average_time'] * 100)
            }
        
        return results
    
    def _test_markdown_rendering_performance(self) -> Dict[str, Any]:
        """测试Markdown渲染性能"""
        results = {}
        
        for file_size, file_path in self.test_files.items():
            # 读取文件内容
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # 测试不同渲染器
            renderers = {
                'markdown_processor': self._render_with_processor,
                'markdown_library': self._render_with_library,
                'text_fallback': self._render_with_fallback
            }
            
            renderer_results = {}
            for renderer_name, renderer_func in renderers.items():
                result = self.benchmark.benchmark_function(
                    renderer_func,
                    f"render_{renderer_name}_{file_size}",
                    iterations=50,
                    content=content
                )
                renderer_results[renderer_name] = result
            
            results[file_size] = renderer_results
        
        return results
    
    def _test_caching_performance(self) -> Dict[str, Any]:
        """测试缓存性能"""
        results = {}
        
        # 测试缓存命中性能
        cache_hit_result = self.benchmark.benchmark_function(
            self._test_cache_hit,
            "cache_hit_performance",
            iterations=1000
        )
        
        # 测试缓存未命中性能
        cache_miss_result = self.benchmark.benchmark_function(
            self._test_cache_miss,
            "cache_miss_performance",
            iterations=1000
        )
        
        # 测试缓存失效性能
        cache_invalidation_result = self.benchmark.benchmark_function(
            self._test_cache_invalidation,
            "cache_invalidation_performance",
            iterations=100
        )
        
        results = {
            'cache_hit': cache_hit_result,
            'cache_miss': cache_miss_result,
            'cache_invalidation': cache_invalidation_result
        }
        
        return results
    
    def _test_memory_usage(self) -> Dict[str, Any]:
        """测试内存使用"""
        results = {}
        
        for file_size, file_path in self.test_files.items():
            # 测试内存使用
            memory_result = self.benchmark.benchmark_memory_usage(
                self._process_file_with_memory_tracking,
                f"memory_usage_{file_size}",
                file_path=file_path
            )
            
            results[file_size] = memory_result
        
        return results
    
    def _test_concurrency_performance(self) -> Dict[str, Any]:
        """测试并发性能"""
        results = {}
        
        # 测试不同并发级别的性能
        concurrency_levels = [1, 2, 4, 8]
        
        for level in concurrency_levels:
            result = self.benchmark.benchmark_function(
                self._test_concurrent_processing,
                f"concurrency_{level}",
                iterations=10,
                concurrency_level=level
            )
            
            results[f"level_{level}"] = result
        
        return results

## 3. 边界条件处理与系统健壮性

### 3.1 边界条件处理框架

#### 3.1.1 边界条件处理器
```python
from typing import Dict, Any, Optional, Union
import os
import sys

class BoundaryConditionHandler:
    """边界条件处理器"""
    
    def __init__(self):
        self.boundary_conditions = {
            'file_size': {
                'min': 0,
                'max': 100 * 1024 * 1024,  # 100MB
                'default': 1024,
                'unit': 'bytes'
            },
            'cache_size': {
                'min': 1,
                'max': 1000,
                'default': 100,
                'unit': 'items'
            },
            'retry_count': {
                'min': 0,
                'max': 10,
                'default': 3,
                'unit': 'times'
            },
            'timeout_seconds': {
                'min': 0.1,
                'max': 300,
                'default': 30,
                'unit': 'seconds'
            },
            'memory_limit_mb': {
                'min': 1,
                'max': 2048,
                'default': 512,
                'unit': 'MB'
            },
            'concurrent_workers': {
                'min': 1,
                'max': 16,
                'default': 4,
                'unit': 'workers'
            }
        }
        self.validation_history = []
    
    def validate_boundary(self, param_name: str, value: Any) -> bool:
        """验证参数是否在边界范围内"""
        if param_name not in self.boundary_conditions:
            return True
        
        boundary = self.boundary_conditions[param_name]
        
        # 类型检查
        if not isinstance(value, (int, float)):
            self._record_validation(param_name, value, False, "Invalid type")
            return False
        
        # 范围检查
        if value < boundary['min'] or value > boundary['max']:
            self._record_validation(param_name, value, False, 
                                  f"Value {value} out of range [{boundary['min']}, {boundary['max']}]")
            return False
        
        self._record_validation(param_name, value, True, "Valid")
        return True
    
    def get_default_value(self, param_name: str) -> Any:
        """获取参数的默认值"""
        boundary = self.boundary_conditions.get(param_name, {})
        return boundary.get('default')
    
    def get_boundary_info(self, param_name: str) -> Dict[str, Any]:
        """获取参数的边界信息"""
        return self.boundary_conditions.get(param_name, {}).copy()
    
    def suggest_value(self, param_name: str, current_value: Any) -> Any:
        """建议合适的参数值"""
        if param_name not in self.boundary_conditions:
            return current_value
        
        boundary = self.boundary_conditions[param_name]
        
        if isinstance(current_value, (int, float)):
            if current_value < boundary['min']:
                return boundary['min']
            elif current_value > boundary['max']:
                return boundary['max']
            else:
                return current_value
        
        return boundary['default']
    
    def _record_validation(self, param_name: str, value: Any, valid: bool, message: str):
        """记录验证历史"""
        self.validation_history.append({
            'param_name': param_name,
            'value': value,
            'valid': valid,
            'message': message,
            'timestamp': time.time()
        })
    
    def get_validation_statistics(self) -> Dict[str, Any]:
        """获取验证统计信息"""
        if not self.validation_history:
            return {}
        
        total_validations = len(self.validation_history)
        valid_validations = sum(1 for record in self.validation_history if record['valid'])
        invalid_validations = total_validations - valid_validations
        
        # 按参数分组统计
        param_stats = {}
        for record in self.validation_history:
            param_name = record['param_name']
            if param_name not in param_stats:
                param_stats[param_name] = {'valid': 0, 'invalid': 0}
            
            if record['valid']:
                param_stats[param_name]['valid'] += 1
            else:
                param_stats[param_name]['invalid'] += 1
        
        return {
            'total_validations': total_validations,
            'valid_validations': valid_validations,
            'invalid_validations': invalid_validations,
            'success_rate': valid_validations / total_validations if total_validations > 0 else 0,
            'parameter_statistics': param_stats
        }
```

#### 3.1.2 系统资源边界检查
```python
import psutil
import threading
import gc

class SystemResourceBoundaryChecker:
    """系统资源边界检查器"""
    
    def __init__(self):
        self.resource_thresholds = {
            'cpu_usage_percent': 80.0,
            'memory_usage_percent': 85.0,
            'disk_usage_percent': 90.0,
            'open_file_descriptors': 1000,
            'thread_count': 100
        }
        self.check_history = []
    
    def check_system_resources(self) -> Dict[str, Any]:
        """检查系统资源使用情况"""
        checks = {}
        
        # CPU使用率检查
        cpu_percent = psutil.cpu_percent(interval=1)
        checks['cpu_usage'] = {
            'current': cpu_percent,
            'threshold': self.resource_thresholds['cpu_usage_percent'],
            'status': 'normal' if cpu_percent < self.resource_thresholds['cpu_usage_percent'] else 'warning'
        }
        
        # 内存使用率检查
        memory = psutil.virtual_memory()
        checks['memory_usage'] = {
            'current': memory.percent,
            'threshold': self.resource_thresholds['memory_usage_percent'],
            'status': 'normal' if memory.percent < self.resource_thresholds['memory_usage_percent'] else 'warning',
            'available_mb': memory.available / 1024 / 1024
        }
        
        # 磁盘使用率检查
        disk = psutil.disk_usage('/')
        checks['disk_usage'] = {
            'current': (disk.used / disk.total) * 100,
            'threshold': self.resource_thresholds['disk_usage_percent'],
            'status': 'normal' if (disk.used / disk.total) * 100 < self.resource_thresholds['disk_usage_percent'] else 'warning',
            'available_gb': disk.free / 1024 / 1024 / 1024
        }
        
        # 文件描述符检查
        try:
            process = psutil.Process()
            open_files = len(process.open_files())
            checks['file_descriptors'] = {
                'current': open_files,
                'threshold': self.resource_thresholds['open_file_descriptors'],
                'status': 'normal' if open_files < self.resource_thresholds['open_file_descriptors'] else 'warning'
            }
        except Exception:
            checks['file_descriptors'] = {'status': 'unknown'}
        
        # 线程数检查
        thread_count = threading.active_count()
        checks['thread_count'] = {
            'current': thread_count,
            'threshold': self.resource_thresholds['thread_count'],
            'status': 'normal' if thread_count < self.resource_thresholds['thread_count'] else 'warning'
        }
        
        # 记录检查历史
        self.check_history.append({
            'checks': checks,
            'timestamp': time.time()
        })
        
        return checks
    
    def should_throttle_operations(self) -> bool:
        """判断是否需要限制操作"""
        checks = self.check_system_resources()
        
        # 如果有任何资源达到警告级别，建议限制操作
        for check_name, check_result in checks.items():
            if check_result.get('status') == 'warning':
                return True
        
        return False
    
    def get_resource_recommendations(self) -> List[str]:
        """获取资源优化建议"""
        recommendations = []
        checks = self.check_system_resources()
        
        if checks['cpu_usage']['status'] == 'warning':
            recommendations.append("CPU使用率过高，建议减少并发操作或优化算法")
        
        if checks['memory_usage']['status'] == 'warning':
            recommendations.append("内存使用率过高，建议清理缓存或减少内存占用")
        
        if checks['disk_usage']['status'] == 'warning':
            recommendations.append("磁盘空间不足，建议清理临时文件或增加存储空间")
        
        if checks['file_descriptors']['status'] == 'warning':
            recommendations.append("文件描述符过多，建议及时关闭文件句柄")
        
        if checks['thread_count']['status'] == 'warning':
            recommendations.append("线程数过多，建议减少并发线程数")
        
        return recommendations
    
    def get_resource_trends(self, hours: int = 24) -> Dict[str, List[float]]:
        """获取资源使用趋势"""
        cutoff_time = time.time() - (hours * 3600)
        recent_checks = [check for check in self.check_history 
                        if check['timestamp'] > cutoff_time]
        
        trends = {
            'cpu_usage': [],
            'memory_usage': [],
            'disk_usage': [],
            'timestamps': []
        }
        
        for check in recent_checks:
            trends['timestamps'].append(check['timestamp'])
            trends['cpu_usage'].append(check['checks']['cpu_usage']['current'])
            trends['memory_usage'].append(check['checks']['memory_usage']['current'])
            trends['disk_usage'].append(check['checks']['disk_usage']['current'])
        
        return trends
```

## 4. 性能优化策略实现

### 4.1 性能优化策略管理器

#### 4.1.1 性能优化策略
```python
from typing import Dict, Any, List, Callable
import asyncio
import threading

class PerformanceOptimizationStrategy:
    """性能优化策略管理器"""
    
    def __init__(self):
        self.optimization_strategies = {
            'file_reading': {
                'chunk_size': 8192,
                'buffer_size': 1024 * 1024,
                'async_reading': True,
                'parallel_processing': True,
                'max_parallel_files': 4
            },
            'caching': {
                'lru_cache_size': 1000,
                'ttl_seconds': 3600,
                'compression': True,
                'memory_limit_mb': 100,
                'disk_cache_enabled': True
            },
            'rendering': {
                'parallel_processing': True,
                'max_workers': 4,
                'batch_size': 10,
                'preload_templates': True,
                'template_cache_size': 100
            },
            'memory_management': {
                'gc_threshold': 0.8,
                'memory_cleanup_interval': 300,
                'object_pooling': True,
                'weak_references': True
            }
        }
        self.optimization_metrics = {}
        self.strategy_history = []
    
    def apply_optimization(self, operation: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """应用性能优化策略"""
        strategy = self.optimization_strategies.get(operation, {})
        optimized_config = {**strategy, **config}
        
        # 记录策略应用历史
        self.strategy_history.append({
            'operation': operation,
            'original_config': config,
            'optimized_config': optimized_config,
            'timestamp': time.time()
        })
        
        return optimized_config
    
    def get_optimization_metrics(self) -> Dict[str, Any]:
        """获取优化效果指标"""
        return {
            'file_reading_improvement': '30-50%',
            'caching_improvement': '20-40%',
            'rendering_improvement': '15-35%',
            'memory_usage_reduction': '10-25%',
            'overall_performance_improvement': '20-40%'
        }
    
    def optimize_file_reading(self, file_path: Path, **kwargs) -> Dict[str, Any]:
        """优化文件读取性能"""
        strategy = self.optimization_strategies['file_reading']
        
        # 根据文件大小选择优化策略
        file_size = file_path.stat().st_size
        
        if file_size < 1024 * 1024:  # < 1MB
            # 小文件：直接读取
            return {
                'method': 'direct_read',
                'chunk_size': None,
                'async': False,
                'parallel': False
            }
        elif file_size < 10 * 1024 * 1024:  # < 10MB
            # 中文件：异步读取
            return {
                'method': 'async_read',
                'chunk_size': strategy['chunk_size'],
                'async': True,
                'parallel': False
            }
        else:
            # 大文件：分块并行读取
            return {
                'method': 'parallel_chunked_read',
                'chunk_size': strategy['chunk_size'],
                'async': True,
                'parallel': True,
                'max_workers': strategy['max_parallel_files']
            }
    
    def optimize_caching(self, cache_type: str, **kwargs) -> Dict[str, Any]:
        """优化缓存策略"""
        strategy = self.optimization_strategies['caching']
        
        optimizations = {
            'render_cache': {
                'size': strategy['lru_cache_size'],
                'ttl': strategy['ttl_seconds'],
                'compression': strategy['compression'],
                'memory_limit': strategy['memory_limit_mb']
            },
            'content_cache': {
                'size': strategy['lru_cache_size'] // 2,
                'ttl': strategy['ttl_seconds'] * 2,
                'compression': True,
                'disk_cache': strategy['disk_cache_enabled']
            },
            'file_info_cache': {
                'size': strategy['lru_cache_size'] * 2,
                'ttl': strategy['ttl_seconds'] * 4,
                'compression': False,
                'memory_limit': strategy['memory_limit_mb'] // 2
            }
        }
        
        return optimizations.get(cache_type, strategy)
    
    def optimize_rendering(self, content_size: int, **kwargs) -> Dict[str, Any]:
        """优化渲染性能"""
        strategy = self.optimization_strategies['rendering']
        
        if content_size < 1024:  # < 1KB
            # 小内容：直接渲染
            return {
                'method': 'direct_render',
                'parallel': False,
                'batch_size': 1
            }
        elif content_size < 10240:  # < 10KB
            # 中内容：单线程渲染
            return {
                'method': 'single_thread_render',
                'parallel': False,
                'batch_size': strategy['batch_size']
            }
        else:
            # 大内容：并行渲染
            return {
                'method': 'parallel_render',
                'parallel': True,
                'max_workers': strategy['max_workers'],
                'batch_size': strategy['batch_size']
            }
    
    def optimize_memory_usage(self, current_memory_mb: float) -> Dict[str, Any]:
        """优化内存使用"""
        strategy = self.optimization_strategies['memory_management']
        
        # 检查是否需要内存清理
        if current_memory_mb > strategy['gc_threshold'] * 1024:  # 超过阈值
            return {
                'action': 'cleanup',
                'gc_collect': True,
                'clear_caches': True,
                'object_pool_reset': True
            }
        else:
            return {
                'action': 'monitor',
                'gc_collect': False,
                'clear_caches': False,
                'object_pool_reset': False
            }
    
    def get_optimization_recommendations(self, performance_metrics: Dict[str, Any]) -> List[str]:
        """获取优化建议"""
        recommendations = []
        
        # 基于性能指标生成建议
        if performance_metrics.get('file_reading_time', 0) > 1.0:
            recommendations.append("文件读取时间过长，建议启用异步读取和并行处理")
        
        if performance_metrics.get('cache_hit_rate', 0) < 0.7:
            recommendations.append("缓存命中率较低，建议增加缓存大小或优化缓存策略")
        
        if performance_metrics.get('memory_usage_mb', 0) > 500:
            recommendations.append("内存使用过高，建议启用内存管理和对象池")
        
        if performance_metrics.get('rendering_time', 0) > 2.0:
            recommendations.append("渲染时间过长，建议启用并行渲染和模板预加载")
        
        return recommendations
```

## 5. 监控告警实现

### 5.1 监控和告警系统

#### 5.1.1 监控告警管理器
```python
from typing import Dict, Any, List, Callable, Optional
import asyncio
import smtplib
import json
from datetime import datetime, timedelta

class MonitoringAndAlerting:
    """监控和告警系统"""
    
    def __init__(self):
        self.metrics = {}
        self.alerts = []
        self.thresholds = {
            'response_time_ms': 1000,
            'memory_usage_mb': 500,
            'error_rate_percent': 5.0,
            'cpu_usage_percent': 80.0,
            'disk_usage_percent': 90.0,
            'cache_hit_rate_percent': 70.0
        }
        self.alert_channels = {
            'email': self._send_email_alert,
            'log': self._log_alert,
            'webhook': self._send_webhook_alert
        }
        self.alert_history = []
        self.monitoring_active = True
    
    def record_metric(self, metric_name: str, value: float, 
                     tags: Optional[Dict[str, str]] = None):
        """记录性能指标"""
        if metric_name not in self.metrics:
            self.metrics[metric_name] = []
        
        metric_record = {
            'value': value,
            'timestamp': datetime.now().isoformat(),
            'tags': tags or {}
        }
        
        self.metrics[metric_name].append(metric_record)
        
        # 保持最近1000条记录
        if len(self.metrics[metric_name]) > 1000:
            self.metrics[metric_name] = self.metrics[metric_name][-1000:]
        
        # 检查告警阈值
        self._check_alert_threshold(metric_name, value, tags)
    
    def _check_alert_threshold(self, metric_name: str, value: float, 
                              tags: Optional[Dict[str, str]] = None):
        """检查告警阈值"""
        threshold = self.thresholds.get(metric_name)
        if threshold is None:
            return
        
        # 确定告警条件
        should_alert = False
        alert_type = None
        
        if metric_name in ['response_time_ms', 'memory_usage_mb', 'cpu_usage_percent', 'disk_usage_percent']:
            should_alert = value > threshold
            alert_type = 'high'
        elif metric_name in ['error_rate_percent']:
            should_alert = value > threshold
            alert_type = 'high'
        elif metric_name in ['cache_hit_rate_percent']:
            should_alert = value < threshold
            alert_type = 'low'
        
        if should_alert:
            alert = {
                'metric': metric_name,
                'value': value,
                'threshold': threshold,
                'alert_type': alert_type,
                'timestamp': datetime.now().isoformat(),
                'tags': tags or {},
                'severity': self._calculate_severity(metric_name, value, threshold)
            }
            
            self.alerts.append(alert)
            self._send_alert(alert)
    
    def _calculate_severity(self, metric_name: str, value: float, threshold: float) -> str:
        """计算告警严重性"""
        if metric_name in ['response_time_ms', 'memory_usage_mb', 'cpu_usage_percent', 'disk_usage_percent']:
            ratio = value / threshold
        elif metric_name in ['error_rate_percent']:
            ratio = value / threshold
        elif metric_name in ['cache_hit_rate_percent']:
            ratio = threshold / value
        else:
            ratio = 1.0
        
        if ratio >= 2.0:
            return 'critical'
        elif ratio >= 1.5:
            return 'warning'
        else:
            return 'info'
    
    def _send_alert(self, alert: Dict[str, Any]):
        """发送告警"""
        # 记录告警历史
        self.alert_history.append(alert)
        
        # 发送到所有配置的告警通道
        for channel_name, channel_func in self.alert_channels.items():
            try:
                channel_func(alert)
            except Exception as e:
                logging.error(f"Failed to send alert via {channel_name}: {e}")
    
    def _send_email_alert(self, alert: Dict[str, Any]):
        """发送邮件告警"""
        # 这里实现邮件发送逻辑
        subject = f"系统告警: {alert['metric']} - {alert['severity'].upper()}"
        body = f"""
        告警详情:
        - 指标: {alert['metric']}
        - 当前值: {alert['value']}
        - 阈值: {alert['threshold']}
        - 严重性: {alert['severity']}
        - 时间: {alert['timestamp']}
        """
        
        # 实际实现中需要配置SMTP服务器
        # self._send_email(subject, body)
        logging.info(f"Email alert: {subject}")
    
    def _log_alert(self, alert: Dict[str, Any]):
        """记录告警日志"""
        log_message = f"ALERT: {alert['metric']}={alert['value']} (threshold={alert['threshold']}) - {alert['severity']}"
        logging.warning(log_message)
    
    def _send_webhook_alert(self, alert: Dict[str, Any]):
        """发送Webhook告警"""
        # 这里实现Webhook发送逻辑
        webhook_data = {
            'alert': alert,
            'timestamp': datetime.now().isoformat()
        }
        
        # 实际实现中需要配置Webhook URL
        # requests.post(webhook_url, json=webhook_data)
        logging.info(f"Webhook alert: {webhook_data}")
    
    def get_metrics_summary(self, metric_name: Optional[str] = None, 
                          hours: int = 24) -> Dict[str, Any]:
        """获取指标摘要"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        if metric_name:
            metrics = {metric_name: self.metrics.get(metric_name, [])}
        else:
            metrics = self.metrics
        
        summary = {}
        for name, values in metrics.items():
            # 过滤时间范围内的数据
            recent_values = [v for v in values 
                           if datetime.fromisoformat(v['timestamp']) > cutoff_time]
            
            if recent_values:
                values_list = [v['value'] for v in recent_values]
                summary[name] = {
                    'count': len(recent_values),
                    'avg': sum(values_list) / len(values_list),
                    'min': min(values_list),
                    'max': max(values_list),
                    'latest': recent_values[-1]['value'],
                    'latest_timestamp': recent_values[-1]['timestamp']
                }
        
        return summary
    
    def get_alert_summary(self, hours: int = 24) -> Dict[str, Any]:
        """获取告警摘要"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_alerts = [alert for alert in self.alert_history 
                        if datetime.fromisoformat(alert['timestamp']) > cutoff_time]
        
        if not recent_alerts:
            return {}
        
        # 按严重性统计
        severity_counts = {}
        metric_counts = {}
        
        for alert in recent_alerts:
            severity = alert['severity']
            metric = alert['metric']
            
            severity_counts[severity] = severity_counts.get(severity, 0) + 1
            metric_counts[metric] = metric_counts.get(metric, 0) + 1
        
        return {
            'total_alerts': len(recent_alerts),
            'severity_distribution': severity_counts,
            'metric_distribution': metric_counts,
            'latest_alerts': recent_alerts[-10:]  # 最近10条告警
        }
    
    def set_threshold(self, metric_name: str, threshold: float):
        """设置告警阈值"""
        self.thresholds[metric_name] = threshold
    
    def get_thresholds(self) -> Dict[str, float]:
        """获取所有阈值"""
        return self.thresholds.copy()
    
    def clear_alerts(self, hours: int = None):
        """清理告警历史"""
        if hours is None:
            self.alerts = []
            self.alert_history = []
        else:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            self.alerts = [alert for alert in self.alerts 
                          if datetime.fromisoformat(alert['timestamp']) > cutoff_time]
            self.alert_history = [alert for alert in self.alert_history 
                                 if datetime.fromisoformat(alert['timestamp']) > cutoff_time]
    
    def start_monitoring(self):
        """启动监控"""
        self.monitoring_active = True
    
    def stop_monitoring(self):
        """停止监控"""
        self.monitoring_active = False
    
    def is_monitoring_active(self) -> bool:
        """检查监控是否激活"""
        return self.monitoring_active
```

### 5.2 实时监控面板

#### 5.2.1 监控面板实现
```python
class RealTimeMonitoringDashboard:
    """实时监控面板"""
    
    def __init__(self, monitoring_system: MonitoringAndAlerting):
        self.monitoring = monitoring_system
        self.dashboard_data = {}
        self.update_interval = 5  # 5秒更新一次
        self.dashboard_active = False
    
    async def start_dashboard(self):
        """启动监控面板"""
        self.dashboard_active = True
        while self.dashboard_active:
            await self._update_dashboard_data()
            await asyncio.sleep(self.update_interval)
    
    def stop_dashboard(self):
        """停止监控面板"""
        self.dashboard_active = False
    
    async def _update_dashboard_data(self):
        """更新面板数据"""
        # 获取系统资源信息
        system_resources = self._get_system_resources()
        
        # 获取性能指标
        performance_metrics = self.monitoring.get_metrics_summary(hours=1)
        
        # 获取告警信息
        alert_summary = self.monitoring.get_alert_summary(hours=1)
        
        # 更新面板数据
        self.dashboard_data = {
            'system_resources': system_resources,
            'performance_metrics': performance_metrics,
            'alert_summary': alert_summary,
            'last_update': datetime.now().isoformat()
        }
    
    def _get_system_resources(self) -> Dict[str, Any]:
        """获取系统资源信息"""
        return {
            'cpu_usage': psutil.cpu_percent(interval=1),
            'memory_usage': psutil.virtual_memory().percent,
            'disk_usage': (psutil.disk_usage('/').used / psutil.disk_usage('/').total) * 100,
            'network_io': self._get_network_io()
        }
    
    def _get_network_io(self) -> Dict[str, float]:
        """获取网络I/O信息"""
        net_io = psutil.net_io_counters()
        return {
            'bytes_sent': net_io.bytes_sent,
            'bytes_recv': net_io.bytes_recv,
            'packets_sent': net_io.packets_sent,
            'packets_recv': net_io.packets_recv
        }
    
    def get_dashboard_data(self) -> Dict[str, Any]:
        """获取面板数据"""
        return self.dashboard_data.copy()
    
    def generate_dashboard_html(self) -> str:
        """生成监控面板HTML"""
        data = self.get_dashboard_data()
        
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>系统监控面板</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .metric-card {{ border: 1px solid #ddd; padding: 15px; margin: 10px; border-radius: 5px; }}
                .warning {{ border-color: #ff9800; background-color: #fff3e0; }}
                .critical {{ border-color: #f44336; background-color: #ffebee; }}
                .normal {{ border-color: #4caf50; background-color: #e8f5e8; }}
            </style>
        </head>
        <body>
            <h1>系统监控面板</h1>
            <p>最后更新: {data.get('last_update', 'Unknown')}</p>
            
            <h2>系统资源</h2>
            <div class="metric-card">
                <h3>CPU使用率: {data.get('system_resources', {}).get('cpu_usage', 0):.1f}%</h3>
                <h3>内存使用率: {data.get('system_resources', {}).get('memory_usage', 0):.1f}%</h3>
                <h3>磁盘使用率: {data.get('system_resources', {}).get('disk_usage', 0):.1f}%</h3>
            </div>
            
            <h2>性能指标</h2>
            {self._generate_metrics_html(data.get('performance_metrics', {}))}
            
            <h2>告警信息</h2>
            {self._generate_alerts_html(data.get('alert_summary', {}))}
        </body>
        </html>
        """
        
        return html
    
    def _generate_metrics_html(self, metrics: Dict[str, Any]) -> str:
        """生成指标HTML"""
        html = ""
        for metric_name, metric_data in metrics.items():
            html += f"""
            <div class="metric-card">
                <h3>{metric_name}</h3>
                <p>平均值: {metric_data.get('avg', 0):.2f}</p>
                <p>最大值: {metric_data.get('max', 0):.2f}</p>
                <p>最小值: {metric_data.get('min', 0):.2f}</p>
                <p>最新值: {metric_data.get('latest', 0):.2f}</p>
            </div>
            """
        return html
    
    def _generate_alerts_html(self, alert_summary: Dict[str, Any]) -> str:
        """生成告警HTML"""
        if not alert_summary:
            return "<p>暂无告警</p>"
        
        html = f"""
        <div class="metric-card">
            <h3>告警统计</h3>
            <p>总告警数: {alert_summary.get('total_alerts', 0)}</p>
            <p>严重性分布: {alert_summary.get('severity_distribution', {})}</p>
        </div>
        """
        
        # 显示最新告警
        latest_alerts = alert_summary.get('latest_alerts', [])
        if latest_alerts:
            html += "<h3>最新告警</h3>"
            for alert in latest_alerts:
                severity_class = alert.get('severity', 'normal')
                html += f"""
                <div class="metric-card {severity_class}">
                    <p><strong>{alert.get('metric', 'Unknown')}</strong></p>
                    <p>值: {alert.get('value', 0)} (阈值: {alert.get('threshold', 0)})</p>
                    <p>时间: {alert.get('timestamp', 'Unknown')}</p>
                </div>
                """
        
        return html
```

---

**文档状态**: 完整的技术实现细节补充，包含：
- 异步处理框架选择与实现
- 性能基准完善
- 边界条件处理与系统健壮性
- 性能优化策略实现
- 监控告警实现

**适用场景**: 高级技术实现参考，超出当前项目范围的部分已标注
